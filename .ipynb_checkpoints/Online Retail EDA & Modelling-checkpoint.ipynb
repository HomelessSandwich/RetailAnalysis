{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def css_styling():\n",
    "    styles = open('./styles/custom.css', 'r').read()\n",
    "    return HTML(styles)\n",
    "\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is part of a technical test for Laing O'Rourke.\n",
    "\n",
    "## Problem Statement\n",
    "In this hypothetical situation, the stakeholders of an online retailer would like to know:\n",
    "- What products are successful?\n",
    "- What times of the day/week/year are successful?\n",
    "- Can customers be segmented?\n",
    "- Can customers be targeted more effectively?\n",
    "\n",
    "## Why Solve This Problem\n",
    "Advertising can drastically increase in costs if the targetting in question is not effective. For instance, if the incorrect targetting is used, many people will read and possibly click on your advert but will not purchase anything. This has meant that you have paid for the advertisement to be put in front of a customer, however, it hasn't achived its goal.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "This is a transnational data set which contains all the transactions occurring between **01/12/2010** and **09/12/2011** for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
    "\n",
    "| Feature     | Description                                                                                                                                                 |\n",
    "|-------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| InvoiceNo   | Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. |\n",
    "| StockCode   | Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.                                                         |\n",
    "| Description | Product (item) name. Nominal.                                                                                                                               |\n",
    "| Quantity    | The quantities of each product (item) per transaction. Numeric.                                                                                             |\n",
    "| InvoiceDate | Invice Date and time. Numeric, the day and time when each transaction was generated.                                                                        |\n",
    "| UnitPrice   | Unit price. Numeric, Product price per unit in sterling.                                                                                                    |\n",
    "| CustomerID  | Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.                                                                     |\n",
    "| Country     | Country name. Nominal, the name of the country where each customer resides.                                                                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Algebra\n",
    "import numpy as np\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.cm import get_cmap\n",
    "import folium\n",
    "from pywaffle import Waffle\n",
    "import missingno as msno\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "# Algorithms\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import metrics\n",
    "\n",
    "# Stats\n",
    "from scipy import stats\n",
    "\n",
    "# Helper Functions\n",
    "from HelperFunctions import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "sns.set(style=\"whitegrid\")  # Stylises graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "print(\"SciPy version: {}\". format(scipy.__version__)) \n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "print(\"Seaborn version: {}\". format(sns.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=warning>Please make sure that you are running Python ver. 3.6 or above and that the version print outs are equal to or above the requirements in <b>requirements.txt<\\b>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/Online Retail.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=info>\n",
    "    Here we can see that everything has been read in fine. 33.1MB of memory has been used, meaning no dtype optimisation is needed.\n",
    "    Some of the dtypes for the variables will need to be adjusted, notably, InvoiceDate.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "## Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df[df.columns].isnull().sum() * 100 / df.shape[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=info>\n",
    "    It mainly seems like Customer ID is missing values. This could cause issues. Description is also missing a small percentage of data. In most cases, this won't be an issue.\n",
    "    There isn't a need to remove these rows, as other features witin the row will be useful.\n",
    "<\\div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Duplicates: {df.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that there are `5268` duplicates. Let us remove these as the combination of `InvoiceNo` and `StockCode` should make a transaction unique. In the data spec the following is said.\n",
    "> `InvoiceNo`: Invoice number. Nominal, a 6-digit integral number **uniquely assigned** to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "\n",
    "> `StockCode`: Product (item) code. Nominal, a 5-digit integral number **uniquely assigned** to each distinct product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InvoiceNo\n",
    "`InvoiceNo` can start with the letter `c`, meaning a cancellation. We should remove this letter, so that we can change the data type of the column into an intger based one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cancelled'] = df['InvoiceNo'].apply(lambda x: 'c' in x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the `Cancelled` column is `True` when there was a `c` in `InvoiceNo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['InvoiceNo'] == 'C544679']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove all C from from each `InvoiceNo` and convert the values to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['InvoiceNo'] = df['InvoiceNo'].str.replace('C|c', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_values = check_coerce_problems(series=df['InvoiceNo'], dtype='numeric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some values that begin with 'A'. There is nothing mentioned in the data description that talks about this. To be safe, these values should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['InvoiceNo'].isin(bad_values)]\n",
    "df['InvoiceNo'] = pd.to_numeric(df['InvoiceNo'], errors='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Duplicates: {df.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InvoiceDate\n",
    "Currently, `InvoiceDate` is in a string format. We would like this in a `datetime` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of Cleaning\n",
    "More cleaning of this data isn't that necessary as there are not many missing values.\n",
    "There could be some class inbalances but these can all be resolved easily here. For other more complex data sets, techniques like SMOTE could be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "It is important to extract extra features using data mining. These can suppliment analysis and machine learning performance.\n",
    "## Datetime Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Month'] = df['InvoiceDate'].dt.month\n",
    "df['Day'] = df['InvoiceDate'].dt.day\n",
    "df['Hour'] = df['InvoiceDate'].dt.hour\n",
    "df['Weekday'] = df['InvoiceDate'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Frequency\n",
    "Here we want to know how many invoices a customer has. This is different to the number of transactions a customer has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_freqs = df.groupby('CustomerID')['InvoiceNo'].nunique()\n",
    "customer_freqs.name = 'Frequency'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "## Countries with Most Transations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = df['Country'].value_counts().reset_index()\n",
    "country_counts.columns = ['Country', 'Number of Invoices']\n",
    "pd.DataFrame(country_counts.head(10))  # Turning into a dataframe for aesthetic reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=info>The UK has by far the most invoices. It may be important to omit the UK from some analysis and visualisations.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countries with Least Transations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(country_counts.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Map of Counties with Highest Revenue and Quanitity of Products Sold\n",
    "With the below map, you can select the overlay of data by clicking on the button in the top right of the map. It is highly recommended that you do so, as you will be able to view all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_plotting = df.groupby(by='Country').sum().reset_index()\n",
    "map_plotting = map_plotting[map_plotting['Country'] != 'United Kingdom']\n",
    "\n",
    "geo_data = './data/countries.geojson'\n",
    "\n",
    "m = folium.Map(location=[0, 0], zoom_start=2.5)\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=geo_data,\n",
    "    name='Amount Spent',\n",
    "    data=map_plotting.groupby(by='Country').sum().reset_index(),\n",
    "    columns=['Country', 'TotalPrice'],\n",
    "    key_on='feature.properties.ADMIN',\n",
    "    fill_color='BuPu',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.5,\n",
    "    legend_name='Amount Spent'\n",
    ").add_to(m)\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=geo_data,\n",
    "    name='Quanitity',\n",
    "    data=map_plotting.groupby(by='Country').sum().reset_index(),\n",
    "    columns=['Country', 'Quantity'],\n",
    "    key_on='feature.properties.ADMIN',\n",
    "    fill_color='YlGn',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.5,\n",
    "    legend_name='Quantity',\n",
    "    show=False\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=warning>The UK has been ommitted as to not skew the scales.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "### Transactions per Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = {\n",
    "    \"Easter Monday\": \"2011-04-25\",\n",
    "    \"Spring Bank Holiday\": \"2011-05-30\",\n",
    "    \"Summer Bank Holiday\": \"2011-08-29\",\n",
    "    \"New Years Day\": \"2011-01-01\",\n",
    "    \"Christmas Day\": \"2010-12-25\",\n",
    "    \"Mothering Sunday\": \"2011-04-03\",\n",
    "    \"Father's Day\": \"2011-06-19\"\n",
    "}\n",
    "\n",
    "\n",
    "holidays_df = pd.DataFrame.from_dict(holidays, orient='index').reset_index()\n",
    "holidays_df.columns = ['Holiday Name', 'Date']\n",
    "holidays_df['Date'] = pd.to_datetime(holidays_df['Date'], yearfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = pd.date_range(\n",
    "    start=df['InvoiceDate'].min(),\n",
    "    end=df['InvoiceDate'].max(),\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "plotting = df.set_index('InvoiceDate').groupby(pd.Grouper(freq='D')).count()\n",
    "\n",
    "# Create a 7 day rolling average\n",
    "plotting = plotting.rolling('7D').sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.plot_date(\n",
    "    x=mdates.date2num(plotting.index), y=plotting['InvoiceNo'],\n",
    "    fmt='b-'\n",
    ")\n",
    "\n",
    "# Plot holidays\n",
    "for index, row in holidays_df.iterrows():\n",
    "    ax.axvline(\n",
    "        x=mdates.date2num(row['Date']),\n",
    "        label=row['Holiday Name'],\n",
    "        linestyle='--', c=next(ax._get_lines.prop_cycler)['color']\n",
    "    )\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "ax.set_title('1 Week Rolling Average of Transactions with Time')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Number of Transactions per Week')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=info>\n",
    "    To no suprise, Q4 has the most transactions. It is also intresting to see how there are no transactions during the Christmas/New Year's break.\n",
    "    Furthermore, you can see spikes in transactions just before holidays and with subsequent dips during and after the holiday concludes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revenue per Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = pd.date_range(\n",
    "    start=df['InvoiceDate'].min(),\n",
    "    end=df['InvoiceDate'].max(),\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "# Don't want to use cancelled ivnocies as they have neg. revenue\n",
    "mask = ~df['Cancelled']\n",
    "\n",
    "plotting = df[mask].set_index('InvoiceDate').groupby(pd.Grouper(freq='D')).sum()\n",
    "\n",
    "# Create a 7 day rolling average\n",
    "plotting = plotting.rolling('7D').sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.plot_date(\n",
    "    x=mdates.date2num(plotting.index), y=plotting['TotalPrice'],\n",
    "    fmt='-'\n",
    ")\n",
    "\n",
    "# Plot holidays\n",
    "for index, row in holidays_df.iterrows():\n",
    "    ax.axvline(\n",
    "        x=mdates.date2num(row['Date']),\n",
    "        label=row['Holiday Name'],\n",
    "        linestyle='--', c=next(ax._get_lines.prop_cycler)['color']\n",
    "    )\n",
    "\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "ax.set_title('Weekly Revenue with Time')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Weekly Revenue')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=info>\n",
    "    Again, here you can see spikes in transactions just before holidays and with subsequent dips during and after the holiday concludes.\n",
    "    These dips during the holidays are more apperent with this data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Successful Hours of the Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET DATA \n",
    "hour_counts = df['Hour'].value_counts()\n",
    "\n",
    "hour_counts = pd.DataFrame(hour_counts.sort_index()).reset_index()\n",
    "hour_counts.columns = ['Hour', 'Number of Transactions']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.barplot(x='Hour', y='Number of Transactions', data=hour_counts)\n",
    "\n",
    "ax.set_title('Number of Transactions For Each Hour')\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", rc={\"axes.facecolor\": (0, 0, 0, 0)})  # Stylises graphs\n",
    "\n",
    "df_months = df.copy()  # Use copy to protect the orginal\n",
    "df_months = df_months[df_months['Country'] == 'United Kingdom']\n",
    "df_months['Month'] = df_months['InvoiceDate'].dt.strftime('%b')\n",
    "month_order = [\n",
    "    'Jan', 'Feb', 'Mar',\n",
    "    'Apr', 'May', 'Jun',\n",
    "    'Jul', 'Aug', 'Sep',\n",
    "    'Oct', 'Nov', 'Dec'\n",
    "]\n",
    "\n",
    "# Initialize the FacetGrid object\n",
    "g = sns.FacetGrid(\n",
    "    df_months, row='Month', hue='Month',\n",
    "    aspect=20, height=1,\n",
    "    row_order=month_order\n",
    ")\n",
    "\n",
    "# Draw the densities in a few steps\n",
    "g.map(\n",
    "    sns.kdeplot, 'Hour',\n",
    "    clip_on=False, shade=True,\n",
    "    alpha=1, lw=1.5, bw=0.25\n",
    ")\n",
    "g.map(sns.kdeplot, 'Hour', clip_on=False, color='black', lw=2, bw=0.25)\n",
    "g.map(plt.axhline, y=0, lw=2, clip_on=False)\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(\n",
    "        0, 0.05, label, fontweight=\"bold\", color=color,\n",
    "        ha=\"left\", va=\"center\", transform=ax.transAxes\n",
    "    )\n",
    "\n",
    "g.map(label, \"Hour\")\n",
    "\n",
    "# Set the subplots to overlap\n",
    "g.fig.subplots_adjust(hspace=-0.5)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=info>\n",
    "    Whilst this visualisation looks nice, it doesn't quite capture anything of note. The purpose of this graph is to show how successful times of the day shift as the year goes on.\n",
    "    This graph should be excluded from a report going to a stakeholder.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")  # Stylises graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Transactions per Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE MATPLOTLIB IS UPDATED TO THE LATEST VERSION FOR THIS PLOT OR IT WILL NOT WORK!!!!!\n",
    "\n",
    "# SET DATA \n",
    "weekday_counts = df['Weekday'].value_counts().sort_index().values.tolist()\n",
    "        \n",
    "# CREATE BACKGROUND\n",
    "days = [\n",
    "    'Mon', 'Tues', 'Wed',\n",
    "    'Thurs', 'Fri', 'Sun',\n",
    "    ''\n",
    "]\n",
    "\n",
    "# Angle of each axis in the plot\n",
    "angles = [(n / 6) * 2 * np.pi for n in range(7)]\n",
    "\n",
    "subplot_kw = {\n",
    "    'polar': True\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15), subplot_kw=subplot_kw)\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_rlabel_position(0)\n",
    "\n",
    "plt.xticks(angles, days)\n",
    "plt.yticks(color=\"grey\", size=10)\n",
    "\n",
    "# PLOT\n",
    "weekday_counts = weekday_counts + [weekday_counts[0]]  # Properly loops the circle back\n",
    "\n",
    "ax.plot(angles, weekday_counts, linewidth=1, linestyle='solid', label=\"Number of Transactions\")\n",
    "ax.fill(angles,  weekday_counts, 'orange', alpha=0.1)\n",
    "\n",
    "plt.title(\"Counts of Transactions for Each Weekday\")\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=info>The number of transactions do seem to fluctate throughout the week here. Friday and Sunday see a drop off. It would be intresting to investigate why this happens.</div>\n",
    "<div class=warning>The data does not have any invoices on Saturdays. This is why is has been excluded.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud\n",
    "Here we will try to get a sense of what products are being sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(description for description in df['Description'].dropna())\n",
    "bag_mask = np.array(Image.open('images/bag.png'))\n",
    "\n",
    "world_cloud = WordCloud(\n",
    "    width=1000, height=1000,\n",
    "    background_color='white', mask=bag_mask,\n",
    "    contour_width=3, contour_color='red'\n",
    ").generate(text)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(world_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=info>These transactions would indicate that this retail store sells a variety of things, backing up what is stated in the data spec that <i>many customers of the company are wholesalers</i>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returned Items\n",
    "Some items are not marked as cancelled but have negative quanitity. This would indicate that these were returned. The intrest in looking at the descriptions here is *why were they sent back*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~df['Cancelled'] & (df['Quantity'] <= 0)\n",
    "text = ' '.join(description for description in df[mask]['Description'].dropna())\n",
    "mask = np.array(Image.open('images/fragile.png'))\n",
    "\n",
    "world_cloud = WordCloud(\n",
    "    width=1000, height=1000,\n",
    "    background_color='white', mask=mask,\n",
    "    contour_width=3, contour_color='red'\n",
    ").generate(text)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(world_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=info>These descriptions would indicate that these items have problems with them and highlights the potential to remove these bits of data as they could skew further analysis.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Returned'] = ~df['Cancelled'] & (df['Quantity'] <= 0)\n",
    "print(f\"Number of items returned: {df['Returned'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_customer_freqs = [value for value in customer_freqs.values if value <= 30]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.countplot(\n",
    "    plot_customer_freqs\n",
    ")\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.xlabel('Number of Invoices')\n",
    "plt.title('Frequency of Customers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=info>A large amount of people have only 1 or 2 orders. The majority have under 5 orders.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Customer Spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~df['Cancelled'] & ~df['Returned']\n",
    "prices = df[mask].groupby(by='InvoiceNo').sum()['TotalPrice']\n",
    "prices = [price for price in prices if price < 1000 and price > 0]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.distplot(\n",
    "    prices, kde=False\n",
    ")\n",
    "plt.xlabel('Invoice Price')\n",
    "plt.title('Distribution of Total Invoice Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=info>What is intresting here are the spikes in distributions. In the data spec it is mentioned <i>many customers of the company are wholesalers</i>. These spikes could be prices in which discounts are applied.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Sold Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~df['Cancelled'] & ~df['Returned']\n",
    "product_quantities = df[mask].groupby(by=['StockCode']).sum()['Quantity'].sort_values(ascending=False)\n",
    "product_quantities = pd.DataFrame(product_quantities).reset_index().head(20)\n",
    "product_quantities['Description'] = product_quantities['StockCode'].apply(\n",
    "    lambda x: df[df['StockCode'] == x]['Description'].values[0]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(\n",
    "    x='StockCode', y='Quantity',\n",
    "    data=product_quantities\n",
    ")\n",
    "plt.ylabel('Quantity Sold')\n",
    "plt.xlabel('Stock Code')\n",
    "plt.title('Top Selling Products')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_quantities.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Cancelled and Returned Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['Cancelled'] | df['Returned']\n",
    "product_quantities = df[mask]['StockCode'].value_counts().sort_values(ascending=False)\n",
    "product_quantities = pd.DataFrame(product_quantities).reset_index().head(20)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(\n",
    "    x='index', y='StockCode',\n",
    "    data=product_quantities\n",
    ")\n",
    "plt.xlabel('Stock Code')\n",
    "plt.title('Most Cancelled and Returned Products')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Customer Value\n",
    "\n",
    "In order to analysise and quantify customer value, the approach taken is **RFM**.\n",
    "\n",
    "- **R**ecency - Customers that have purchased more recently are more likely to purchase again.\n",
    "- **F**requency - Customers that have made purchases are more likely to purchase again.\n",
    "- **M**onetary Value - Customers that have spent more are more likely to purchase again.\n",
    "\n",
    "Recency is the most important behaviour in identifying customers that will response well to advertising. Frequency is the second most important, with monetary value being the third most.\n",
    "In this analysis each behaviour will be ranked between 1 and 10. Likely, there is a better scale, for instance, [scikit-learn's Standard Scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). For interpretability for the stakeholder this hasn't been used, at the potential cost of machine learning performance.\n",
    "\n",
    "## Constructing RFM Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monetary Value\n",
    "Here we want to find the maximum invoice (not transaction) that a customer has issued. This is to then be tacked onto the previously generated customer freqeuncy table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['CustomerID', 'TotalPrice', 'Frequency']\n",
    "mask = ~df['Cancelled'] & ~df['Returned']\n",
    "\n",
    "# The long statement first groups on Customer and their Invoice to get their total for an invoice\n",
    "# It then groups on customer to find each customer's max spend on an invoice\n",
    "customers = pd.merge(\n",
    "    df[mask].groupby(by=['CustomerID', 'InvoiceNo']).sum().reset_index().groupby(by='CustomerID').max().reset_index(),\n",
    "    customer_freqs,\n",
    "    on='CustomerID', how='inner'\n",
    ")[keep_cols]\n",
    "\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recency\n",
    "Next, we need to find out how recently, from the start date of the data set, was each invoice issued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Recency'] = (max(df['InvoiceDate']) - df['InvoiceDate']).dt.days\n",
    "keep_cols = ['CustomerID', 'TotalPrice', 'Recency']\n",
    "\n",
    "customers = pd.merge(\n",
    "    df[mask].groupby(by='CustomerID').min()['Recency'].reset_index(),\n",
    "    customers,\n",
    "    on='CustomerID', how='left'\n",
    ")\n",
    "customers.columns = ['ID', 'Recency', 'Monetary Value', 'Frequency']\n",
    "\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "Removing outliers here is extremely important. This is because when asigning a 1 - 10 score for each behaviour, we don't want the distribution of scores to be heavily skewed to one side. Outliers can also amplify a strong correlation or make an otherwise strong correlation appear weak.\n",
    "\n",
    "Using z-scores could be effective here, however, it is likely that the data is nonparametric. If the data is nonparametric, Dbscan and Isolation Forests can be good solutions.\n",
    "Using interquatile range is a good rule of thumb and is easier to explain to a stakeholder. This could impact machine learning performance but doesn't take too much time to implement.\n",
    "With more time, class inbalance correcting techniques and the aforementioned outlier removal methods could be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cols = [\n",
    "    'Monetary Value', 'Frequency'\n",
    "]\n",
    "\n",
    "outliers_df = pd.DataFrame(columns=customers.columns)\n",
    "\n",
    "for col in outlier_cols:\n",
    "    stat = customers[col].describe()\n",
    "    print(stat)\n",
    "    IQR = stat['75%'] - stat['25%']\n",
    "    upper = stat['75%'] + 1.5 * IQR\n",
    "    lower = stat['25%'] - 1.5 * IQR\n",
    "    \n",
    "    outliers = customers[(customers[col] > upper) | (customers[col] < lower)]\n",
    "\n",
    "    if not outliers.empty:\n",
    "        print(f'\\nOutlier found in: {col}')\n",
    "        outliers_df = pd.concat([outliers_df, outliers])\n",
    "    else:\n",
    "        print(f'\\nNo outlier found in: {col}')\n",
    "\n",
    "    print(f'\\nSuspected Outliers Lower Bound: {lower}')\n",
    "    print(f'Suspected Outliers Upper Bound: {upper}\\n\\n')\n",
    "\n",
    "print(f'Number of outlier rows: {len(outliers_df)}')\n",
    "\n",
    "del outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers[~customers['ID'].isin(outliers_df['ID'].values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFM Scaling\n",
    "Here the RFM values are scaled between 1 and 10. With more time, sklearn's standard scaler amongst other methods could be implemented here for better machine learning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['Recency'] = np.interp(customers['Recency'], (customers['Recency'].min(), customers['Recency'].max()), (10, 0))\n",
    "customers['Frequency'] = np.interp(customers['Frequency'], (customers['Frequency'].min(), customers['Frequency'].max()), (0, 10))\n",
    "customers['Monetary Value'] = np.interp(customers['Monetary Value'], (customers['Monetary Value'].min(), customers['Monetary Value'].max()), (0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.distplot(\n",
    "    customers['Recency'], kde=False, bins=10\n",
    ")\n",
    "plt.xlabel('Recency')\n",
    "plt.title('Distribution of Customer Recencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.distplot(\n",
    "    customers['Frequency'], kde=False, bins=10\n",
    ")\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Distribution of Customer Frequencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.distplot(\n",
    "    customers['Monetary Value'], kde=False, bins=10\n",
    ")\n",
    "plt.xlabel('Monetary Value')\n",
    "plt.title('Distribution of Customer Monetary Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=info>All three values for RFM experience both negative and positive skews here. This should be kept in mind.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "## Model Explanations\n",
    "![alt-text](https://bigdataldn.com/wp-content/uploads/2017/05/2-variable-clustering.jpg)\n",
    "\n",
    "An unsupervised machine learning model will need to be implented. Unsupervised learning is used on data sets that do not have labelled responses to look for previously undetected patterns in a data set, where it is not feasible to be done by hand.\n",
    "### K Means Clustering\n",
    "K Means clustering is perhaps the most used unsupervised machine learning technique. First, ***k*** clusters are chosen. A random initial centre for each cluster is chosen. The next two steps are applied a number of times:\n",
    "1. Assign each observation to its nearest centre\n",
    "2. Update the centre of each cluster based off the centre of respective observation\n",
    "\n",
    "![alt-text](https://miro.medium.com/max/700/1*oj1MBxiPfnyeQC3HnCcJiw.gif)\n",
    "\n",
    "### Hierarchical Clustering\n",
    "With this technique, initially each data point is made to be its own cluster. With each iteration, similar clusters marget with one and another until ***k*** clusters are formed. The following steps are computed:\n",
    "1. Compute the proximity matrix\n",
    "2. Let each data point be a cluster\n",
    "3. Merge the two closest clusters and update the proximity matrix - repeat until one cluster remains\n",
    "\n",
    "With this, any number ***k*** clusters can be seleted.\n",
    "\n",
    "Here we will only be using *agglomerative* clustering. With more time, it could be worth seeing how the other form (*divisive*) of hierarchical clustering performs.\n",
    "\n",
    "![alt-text](https://dashee87.github.io/images/hierarch.gif)\n",
    "\n",
    "#### Average Linkage\n",
    "![alt-text](https://www.saedsayad.com/images/Clustering_average.png)\n",
    "\n",
    "The distance between each point in one cluster to every point in the other cluster is averaged. Average linking does well if there is noise between clusters but is bias towards globular clusters.\n",
    "\n",
    "#### Single Linkage\n",
    "![alt-text](https://www.saedsayad.com/images/Clustering_single.png)\n",
    "\n",
    "The *shortest* distance between two points in each cluster. Single linkage can separate non-elliptical shapes, providing the gap between a pair of clusters is not small. However, it cannot separate clusters efficiently if there is noise between clusters.\n",
    "\n",
    "#### Complete Linkage\n",
    "![alt-text](https://www.saedsayad.com/images/Clustering_complete.png)\n",
    "\n",
    "The *longest* distance between two points in each cluster. In a stark contrast to single linkage, complete linkage excels in separating clusters if there is noise between clusters. However, like average linkage, it tents to be biased towards globular clusters and tends to break large clusters.\n",
    "\n",
    "#### Ward's Method\n",
    "![alt-text](https://miro.medium.com/max/491/1*2AYd0CXANWsM8MLwmrJzYQ.jpeg)\n",
    "\n",
    "Ward's method is the same as average linkage, except it calcuates the sum of the square distances. It shares the same pros and cons of average linkage too.\n",
    "\n",
    "---\n",
    "\n",
    "With more time, the effectiveness of ***Spectral Clustering*** and ***Gaussian mixture models*** could be tested.\n",
    "\n",
    "## Hyperparamter Tuning\n",
    "One upside of the hyperparamter tuning, is that supervised learning methods usually have many more hyperparameters to tune.\n",
    "\n",
    "For example, here is the *elbow technique* to find an optimal number of clusters for our data. To find the number of clusters you simply pick the *elbow* of the curve. It is not the most objective method of finding the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "n_clusters = range(2, 11)\n",
    "\n",
    "for n in n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n, random_state=0)\n",
    "    kmeans.fit(customers.drop(columns=['ID']))\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(\n",
    "    n_clusters, inertias\n",
    ")\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Distribution of Customer Monetary Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate each method, we will be using *silhouette scoring* and *calinski harabaz indexing*.\n",
    "\n",
    "### Silhouette Scoring\n",
    "A silhouette score describes how similar a datapoint is to other datapoints within its own cluster, relative to datapoints not in its cluster. This is computed for every data point.\n",
    "This score is bounded between -1 and 1.\n",
    "\n",
    "A score of 1 suggests each cluster is very tight and -1 suggests that each cluster is almost random.\n",
    "\n",
    "### Calinski Harabasz Indexing\n",
    "A Calinski-Harabasz index describes the ratio of the comparison of variance of a datapoint compared to points in its own clusters against the points in other clusters. A high index score is good here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {'Cluster': [], 'N Clusters': [], 'Silhouette Score': [], 'Calinski Harabasz Score': []}\n",
    "names = ['kmeans', 'ward', 'complete', 'average', 'single']\n",
    "n_clusters = range(2, 11)\n",
    "\n",
    "for n in n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n, random_state=0).fit(customers.drop(columns=['ID']))\n",
    "    ward = AgglomerativeClustering(n_clusters=n, linkage='ward').fit(customers.drop(columns=['ID']))\n",
    "    complete = AgglomerativeClustering(n_clusters=n, linkage='complete').fit(customers.drop(columns=['ID']))\n",
    "    average = AgglomerativeClustering(n_clusters=n, linkage='average').fit(customers.drop(columns=['ID']))\n",
    "    single = AgglomerativeClustering(n_clusters=n, linkage='single').fit(customers.drop(columns=['ID']))\n",
    "    \n",
    "    clusters = [kmeans, ward, complete, average, single]\n",
    "    \n",
    "    for cluster, name in zip(clusters, names):\n",
    "            silhouett_score = metrics.silhouette_score(customers.drop(columns=['ID']), cluster.labels_)\n",
    "            calinski_harabasz_score = metrics.calinski_harabasz_score(customers.drop(columns=['ID']), cluster.labels_)\n",
    "\n",
    "            scores['Cluster'].append(name)\n",
    "            scores['N Clusters'].append(n)\n",
    "            scores['Silhouette Score'].append(silhouett_score)\n",
    "            scores['Calinski Harabasz Score'].append(calinski_harabasz_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(scores)\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for cluster in names:\n",
    "    filter_cluster = scores[scores['Cluster'] == cluster]['Silhouette Score']\n",
    "    plt.plot(\n",
    "        n_clusters, filter_cluster,\n",
    "        label=cluster\n",
    "    )\n",
    "\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores of Models with Number of Clusters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for cluster in names:\n",
    "    filter_cluster = scores[scores['Cluster'] == cluster]['Calinski Harabasz Score']\n",
    "    plt.plot(\n",
    "        n_clusters, filter_cluster,\n",
    "        label=cluster\n",
    "    )\n",
    "\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Calinski Harabasz Score')\n",
    "plt.title('Calinski Harabasz Scores of Models with Number of Clusters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = KMeans(n_clusters=3, random_state=0).fit(customers.drop(columns=['ID']))\n",
    "customers['Cluster Label'] = final_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single linkage hierarchical model with 2 clusters offers the highest silhouette score, however, its Calinski Harabasz index is ***very*** low. For this reason, a kmeans clustering algorithm has been chosen with 3 clusters. It offers an acceptable silhouette and the highest Calinski Harabasz index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = Axes3D(fig)\n",
    "label_hues = {\n",
    "    0: 'tab:green',\n",
    "    1: 'tab:red',\n",
    "    2: 'tab:blue'\n",
    "}\n",
    "\n",
    "for cluster_label, centre in zip(customers['Cluster Label'].unique(), kmeans.cluster_centers_):\n",
    "    # Plot customer data with cluster label colouring\n",
    "    ax.scatter(\n",
    "        customers[customers['Cluster Label'] == cluster_label]['Recency'],\n",
    "        customers[customers['Cluster Label'] == cluster_label]['Monetary Value'],\n",
    "        customers[customers['Cluster Label'] == cluster_label]['Frequency'],\n",
    "        c=label_hues[cluster_label], label=f'Customer Group: {cluster_label}'\n",
    "    )\n",
    "    \n",
    "    # Plot the centre of cluster\n",
    "    ax.scatter(\n",
    "        centre[0], centre[1], centre[2],\n",
    "        c='black', marker='x',\n",
    "        s=120, linewidths=3,\n",
    "    ) \n",
    "\n",
    "ax.view_init(elev=23, azim=160)\n",
    "ax.set_xlabel('Recency Score')\n",
    "ax.set_ylabel('Monetary Value Score')\n",
    "ax.set_zlabel('Frequency Score')\n",
    "ax.set_title('KMeans Clustering of RFM Customer Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(x='Cluster Label', y='Recency', palette=label_hues, data=customers)\n",
    "plt.title('Box Plot of Customer Recency Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(x='Cluster Label', y='Monetary Value', palette=label_hues, data=customers)\n",
    "plt.title('Box Plot of Customer Monetary Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(x='Cluster Label', y='Frequency', palette=label_hues, data=customers)\n",
    "plt.title('Box Plot of Customer Frequency Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_groups = customers.groupby('Cluster Label').count()['ID']\n",
    "model_groups = round(model_groups / model_groups.sum(), 2) * 100\n",
    "\n",
    "legend = {\n",
    "    'labels': [f\"Customer Label: {k} ({v}%)\" for k, v in model_groups.items()],\n",
    "    'loc': 'lower left',\n",
    "    'bbox_to_anchor': (0, -0.1),\n",
    "    'ncol': len(customers),\n",
    "    'framealpha': 0,\n",
    "    'fontsize': 12\n",
    "}\n",
    "title = {\n",
    "    'label': 'Percentage of Customer Groups',\n",
    "    'loc': 'left',\n",
    "    'fontdict': {'fontsize': 20}\n",
    "}\n",
    "\n",
    "fig = plt.figure(\n",
    "    FigureClass=Waffle,\n",
    "    rows=5, columns=10,\n",
    "    values=model_groups,\n",
    "    icons=['user', 'user', 'user'],\n",
    "    font_size=20, icon_style='solid', icon_legend=True,\n",
    "    figsize=(10, 10), legend=legend, title=title,\n",
    "    colors=list(label_hues.values())\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_country(ID):\n",
    "    countries = df[df['CustomerID'] == ID]['Country'].unique()\n",
    "    \n",
    "    if len(countries) > 0:\n",
    "        return countries[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "customer_countries = {'ID': [], 'Country': []}\n",
    "\n",
    "for ID in df['CustomerID'].unique():\n",
    "    customer_countries['ID'].append(ID)\n",
    "    customer_countries['Country'].append(customer_country(ID))\n",
    "    \n",
    "customer_countries = pd.DataFrame(customer_countries)\n",
    "\n",
    "customers = pd.merge(\n",
    "    customers,\n",
    "    customer_countries,\n",
    "    on='ID', how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['Total'] = customers['Recency'] + customers['Monetary Value'] + customers['Frequency']\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = {'Country': [], 0: [], 1: [], 2: [], 'Total': []}\n",
    "\n",
    "for country in customers['Country'].unique():\n",
    "    counts = customers[customers['Country'] == country]['Cluster Label'].value_counts()\n",
    "    \n",
    "    countries['Country'].append(country)\n",
    "    countries['Total'].append(counts.sum())\n",
    "    \n",
    "    for cluster_label in [0, 1, 2]:\n",
    "        if cluster_label in counts:\n",
    "            countries[cluster_label].append(counts[cluster_label])\n",
    "        else:\n",
    "            countries[cluster_label].append(0)\n",
    "countries = pd.DataFrame(countries)\n",
    "countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[0, 0], zoom_start=2.5)\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=geo_data,\n",
    "    name='Average Customer Score',\n",
    "    data=customers.groupby(by='Country').mean()['Total'].reset_index(),\n",
    "    columns=['Country', 'Total'],\n",
    "    key_on='feature.properties.ADMIN',\n",
    "    fill_color='BuPu',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.5,\n",
    "    legend_name='Average Customer Score'\n",
    ").add_to(m)\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=geo_data,\n",
    "    name='Number of Customers',\n",
    "    data=countries[countries['Country'] != 'United Kingdom'],\n",
    "    columns=['Country', 'Total'],\n",
    "    key_on='feature.properties.ADMIN',\n",
    "    fill_color='BuPu',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.5,\n",
    "    legend_name='Number of Customers',\n",
    "    show=False\n",
    ").add_to(m)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(\n",
    "    x='Total', y='Country',\n",
    "    data=countries[countries['Country'] != 'United Kingdom'].sort_values(ascending=False, by='Total')\n",
    ")\n",
    "plt.xlabel('Total Customers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_plotting = countries[countries['Total'] >= 5].copy()\n",
    "\n",
    "totals = [i + j + k for i, j, k in zip(c_plotting[0], c_plotting[1], c_plotting[2])]\n",
    "zero_bars = [i / j * 100 for i, j in zip(c_plotting[0], totals)]\n",
    "one_bars = [i / j * 100 for i, j in zip(c_plotting[1], totals)]\n",
    "two_bars = [i / j * 100 for i, j in zip(c_plotting[2], totals)]\n",
    "\n",
    "c_plotting['Total'] = totals\n",
    "c_plotting[0] = zero_bars\n",
    "c_plotting[1] = one_bars\n",
    "c_plotting[2] = two_bars\n",
    "\n",
    "c_plotting = c_plotting.sort_values(by=1, ascending=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "barWidth = 0.85\n",
    "\n",
    "plt.bar(\n",
    "    range(len(totals)), c_plotting[0],\n",
    "    color='tab:green', edgecolor='white',\n",
    "    width=barWidth, label='Customer Label: 0'\n",
    ")\n",
    "\n",
    "plt.bar(\n",
    "    range(len(totals)), c_plotting[1], bottom=c_plotting[0],\n",
    "    color='tab:red', edgecolor='white',\n",
    "    width=barWidth, label='Customer Label: 1'\n",
    ")\n",
    "\n",
    "plt.bar(\n",
    "    range(len(totals)), c_plotting[2],\n",
    "    bottom=[i + j for i, j in zip(c_plotting[0], c_plotting[1])],\n",
    "    color='tab:blue', edgecolor='white',\n",
    "    width=barWidth, label='Customer Label: 2'\n",
    ")\n",
    " \n",
    "\n",
    "plt.xticks(range(len(totals)), c_plotting['Country'].unique())\n",
    "\n",
    "plt.xlabel('Country')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.ylabel('Customer Label Share %')\n",
    "\n",
    "plt.title('100% Stacked Bar Chart of Customer Labels with Each Country That Has >= 5 Customers')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~df['Cancelled'] & ~df['Returned'] & df['CustomerID'].isin(customers[customers['Cluster Label'] == 2]['ID'].values)\n",
    "product_quantities = df[mask].groupby(by=['StockCode']).sum()['Quantity'].sort_values(ascending=False)\n",
    "product_quantities = pd.DataFrame(product_quantities).reset_index().head(20)\n",
    "product_quantities['Description'] = product_quantities['StockCode'].apply(\n",
    "    lambda x: df[df['StockCode'] == x]['Description'].values[0]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(\n",
    "    x='StockCode', y='Quantity',\n",
    "    data=product_quantities\n",
    ")\n",
    "plt.ylabel('Quantity Sold')\n",
    "plt.xlabel('Stock Code')\n",
    "plt.title('Top Selling Products Among Customers with a Customer Label of 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_quantities.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights and Decisions\n",
    "\n",
    "Cluster label `0`: These seem to be customers with that have recently bought products but don't have a high frequency and didn't buy anything expensive.\n",
    "\n",
    "Cluster label `1`: These customers have low recency, frequency and monetary value. They have little value to the company.\n",
    "\n",
    "Cluster label `2`: These customers are recent buyers with a wide range of freqencies and monetary values. These customers are of the most interest to the company.\n",
    "\n",
    "It would make sense to focus efforts in targetting customers with a label of `2` as they boast high recency scores. Recency scoring being the most important behaviour. Along with this, customers with a label of `2` generally have greater frequency and monetary scores than that of customers with a label of `0` and `1`. A large majority of customers have a label of `0`. It could be argued that these also need to be targeted, in order to increase the frequency in which they purchase items.\n",
    "\n",
    "The UK should be the focus of the majority of advertisement spend, however, France and Germany already have a comparitivley large amount of customers. Germany does have a smaller percentage of customers with a label of `1`. Advertising could be quite effective there.\n",
    "\n",
    "Customers with a label of `2` seem to purchase differing items than that of the general customer base. The following would be effective items to be displayed within an advertisement towards these customers:\n",
    "- WORLD WAR 2 GLIDERS ASSTD DESIGNS\n",
    "- SMALL CHINESE STYLE SCISSOR\n",
    "- ASSORTED COLOUR BIRD ORNAMENT\n",
    "- JUMBO BAG RED RETROSPOT\n",
    "- WHITE HANGING HEART T-LIGHT HOLDER\n",
    "\n",
    "Advertising to these customers would be most effective midday, during the working week and closer towards the end of the year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "The main next step would be to test other methods of:\n",
    "- Class imbalances\n",
    "- Outlier detection techniques\n",
    "- Methods of scaling RFM values\n",
    "- Unsupervised machine learning models\n",
    "- Evaluative unsupervised machine learning methods\n",
    "\n",
    "Testing all of these would undoubtably increase model performance.\n",
    "\n",
    "Another key aspects of this process is the deployment, maintenance and updating of the model.\n",
    "It would be key to work alongside a data architect to ensure that the flow of all three aspects are achieved.\n",
    "New data would improve the performance of machine learning models, so that new versions of it can be deployed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
